import pandas as pd
import numpy as np
import streamlit as st
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import geopandas as gpd
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
import statsmodels.api as sm


from functii import detect_outliers_iqr, plot_boxplots,assign_region

data = pd.read_excel('dataIN/fashion.xlsx',sheet_name='Sheet1',index_col=0)
data["Annual Income (k$)"] = data["Price"] * 10 + 30
data["Spending Score"] = data["Sales Volume"] + np.random.randint(-10, 10, size=len(data))
data["Genre"] = np.random.choice(["Male", "Female"], size=len(data))
data["Age"] = np.random.randint(18, 65, size=len(data))

st.markdown("""
<style>
.title{
    color:#fa82f0 !important;
}
.custom-title {
    color: #b5029d !important;
}
</style>
""", unsafe_allow_html=True)

st.markdown('<h1 class="title">Proiect Analizare VÃ¢nzÄƒri Fashion</h1>',unsafe_allow_html=True)
st.markdown('<h3 class="custom-title">Antonescu Gabriela & Guguloi Iulia</h3>', unsafe_allow_html=True)

# Meniu de navigare lateral
section = st.sidebar.radio("NavigaÈ›i la:",
                           ["Descriere dataset",
                            "Informatii privind DataFrame-ul",
                            "Valori lipsa ale datasetului",
                            "Tratare valori lipsa",
                            "Analiza valorilor extreme",
                            "Codificare È™i scalare",
                            "Identificare si stergere duplicate",
                            "Grupare dupa Categorie",
                            "Predictie pret",
                            "AnalizÄƒ GeograficÄƒ",
                            "Clusterizare clienÈ›i",
                            "Clasificare logisticÄƒ",
                            "Regresie multiplÄƒ",
                            ], key="radio_navigare")

if section == "Descriere dataset":
    st.header("ðŸ“Š Analiza datasetului Fashion Sales")
    st.markdown("""
    ### ðŸ›ï¸ Despre Fashion Sales Dataset
    **Fashion Sales Dataset** este un set de date detaliat, conceput pentru a simula procesul real de vÃ¢nzÄƒri Ã®n industria modei. Acesta oferÄƒ o perspectivÄƒ valoroasÄƒ asupra pieÈ›ei modei È™i reprezintÄƒ o resursÄƒ esenÈ›ialÄƒ pentru analiza tendinÈ›elor de vÃ¢nzare, prognoza cererii È™i optimizarea strategiilor de afaceri.

    ðŸ“Œ **Caracteristici principale:**
    - ðŸ›’ **Proces de vÃ¢nzÄƒri realist** â€“ Include detalii despre achiziÈ›iile clienÈ›ilor, tranzacÈ›ii È™i caracteristicile produselor.
    - ðŸ“† **ActualizÄƒri sÄƒptÄƒmÃ¢nale** â€“ ReflectÄƒ cele mai recente tendinÈ›e È™i preferinÈ›e ale consumatorilor.
    - ðŸ·ï¸ **Diverse atribute** â€“ Cuprinde informaÈ›ii despre preÈ›uri, branduri, categorii, rating-uri, review-uri, dimensiuni disponibile, culori È™i istoric de achiziÈ›ii.
    - ðŸ§‘â€ðŸ¤â€ðŸ§‘ **InteracÈ›iuni cu clienÈ›ii** â€“ ConÈ›ine date despre reviste de modÄƒ, influenceri È™i impactul reÈ›elelor sociale asupra deciziilor de cumpÄƒrare.
    - ðŸ‚ **VariaÈ›ii sezoniere** â€“ Permite analiza preferinÈ›elor de modÄƒ Ã®n funcÈ›ie de anotimpuri È™i perioade specifice din zi.
    - ðŸ“Š **EÈ™antion semnificativ** â€“ NumÄƒrul mare de observaÈ›ii asigurÄƒ un set de date robust pentru analize detaliate È™i prognoze precise.
    """)

if section == "Informatii privind DataFrame-ul":
    st.header('Dataset')
    st.dataframe(data.head(1000))

if section == "Valori lipsa ale datasetului":
    if data.isnull().sum().sum() == 0:
        st.success("âœ… Nu existÄƒ valori lipsÄƒ Ã®n dataset!")
    else:
        st.warning("âš ï¸ ExistÄƒ valori lipsÄƒ!")
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.heatmap(data.isnull(), cbar=False, cmap="viridis", ax=ax)
        st.pyplot(fig)

if section == "Tratare valori lipsa":
    st.header("ðŸ§¼ Tratarea valorilor lipsÄƒ")
    missing = data.isnull().sum()
    missing_percent = (missing / len(data)) * 100
    missing_df = pd.DataFrame({"Valori LipsÄƒ": missing, "Procent": missing_percent})
    missing_df = missing_df[missing_df["Valori LipsÄƒ"] > 0]
    st.dataframe(missing_df)

    data_imputed = data.copy()
    for col in data.columns:
        if data[col].isnull().sum() > 0:
            if data[col].dtype == 'object':
                data_imputed[col] = data[col].fillna(data[col].mode()[0])
            else:
                data_imputed[col] = data[col].fillna(data[col].median())

    st.success("âœ… Valorile lipsa au fost completate (mediana sau moda, dupa tipul coloanei)")
    st.dataframe(data_imputed.head())

if section == "Analiza valorilor extreme":
    st.title("ðŸ“Š Analiza Valorilor Extreme - Fashion Sales")

    st.markdown("""
    **Ce sunt valorile extreme?**  
    Valorile extreme sunt punctele de date care sunt semnificativ mai mari sau mai mici decÃ¢t restul observaÈ›iilor. Acestea pot afecta analiza statisticÄƒ È™i rezultatele modelului nostru, de aceea este important sÄƒ le gestionÄƒm corespunzÄƒtor.

    **Cum le detectÄƒm?**  
    Folosim metoda **Interquartile Range (IQR)**, care identificÄƒ valori extreme ca fiind cele care depÄƒÈ™esc limita inferioarÄƒ È™i superioarÄƒ:

    - **Q1 (quartila 25%)** â€“ valoarea sub care se aflÄƒ 25% dintre date
    - **Q3 (quartila 75%)** â€“ valoarea sub care se aflÄƒ 75% dintre date
    - **IQR = Q3 - Q1**
    - **Limita inferioarÄƒ** = Q1 - 1.5 * IQR
    - **Limita superioarÄƒ** = Q3 + 1.5 * IQR

    Orice valoare Ã®n afara acestui interval este consideratÄƒ **extremÄƒ**.
    """)
    sales_lb, sales_ub = detect_outliers_iqr(data, "Sales Volume")
    price_lb, price_ub = detect_outliers_iqr(data, "Price")

    st.write(f"ðŸ”¹ **Limite pentru `Sales Volume`**: [{sales_lb:.2f}, {sales_ub:.2f}]")
    st.write(f"ðŸ”¹ **Limite pentru `Price`**: [{price_lb:.2f}, {price_ub:.2f}]")

    #pastram datele
    var1 = data.copy()

    #eliminam valorile extreme
    var2 = data[(data["Sales Volume"] >= sales_lb) & (data["Sales Volume"] <= sales_ub) &
                (data["Price"] >= price_lb) & (data["Price"] <= price_ub)]

    #inlocuim valorile extreme cu percentila 95%
    var3 = data.copy()
    var3.loc[var3["Sales Volume"] > sales_ub, "Sales Volume"] = data["Sales Volume"].quantile(0.95)
    var3.loc[var3["Price"] > price_ub, "Price"] = data["Price"].quantile(0.95)

    option = st.radio(
        "Alege varianta de tratare a valorilor extreme:",
        ("Original", "Fara Valori Extreme", "Inlocuire cu Percentila 95%"),
        key="radio_analiza_valori_extreme"
    )

    if option == "Original":
        st.markdown("""
        **Varianta `Original` pastreaza toate datele aÈ™a cum sunt, fara a elimina sau modifica valori extreme.**
        Aceasta poate fi utila atunci cand vrem sa vedem intreaga distributie a datelor.
        """)
        st.pyplot(plot_boxplots(var1, "Original"))
        st.write(f"ðŸ“Š Numar total de observaÈ›ii: {len(var1)}")

    elif option == "Fara Valori Extreme":
        st.markdown("""
        **Varianta `FÄƒrÄƒ Valori Extreme` eliminÄƒ toate observaÈ›iile care depÄƒÈ™esc limitele definite prin metoda IQR.**
        Aceasta este utilÄƒ dacÄƒ dorim sÄƒ avem o distribuÈ›ie mai curatÄƒ a datelor, fÄƒrÄƒ valori care ar putea distorsiona analiza.
        """)
        st.pyplot(plot_boxplots(var2, "FÄƒrÄƒ Valori Extreme"))
        st.write(f"ðŸ“‰ NumÄƒr de observaÈ›ii dupÄƒ eliminare: {len(var2)}")
        st.write(f"ðŸš€ ObservaÈ›ii eliminate: {len(var1) - len(var2)}")

    elif option == "Inlocuire cu Percentila 95%":
        st.markdown("""
        **Varianta `ÃŽnlocuire cu Percentila 95%` modificÄƒ valorile extreme peste limitÄƒ cu valoarea percentilei 95%.**
        Aceasta este o metodÄƒ mai puÈ›in agresivÄƒ decÃ¢t eliminarea completÄƒ a datelor È™i poate fi utilÄƒ atunci cÃ¢nd dorim sÄƒ pÄƒstrÄƒm informaÈ›ia generalÄƒ.
        """)
        st.pyplot(plot_boxplots(var3, "ÃŽnlocuire cu Percentila 95%"))
        st.write(f"âœ… ÃŽnlocuire realizatÄƒ pentru valorile extreme peste percentila 95%")

    st.subheader("ðŸ“Š Statistici descriptive")
    st.write("ðŸ”¹ **Original**")
    st.write(var1[["Sales Volume", "Price"]].describe())

    if option != "Original":
        st.write("ðŸ”¹ **Noua varianta de date**")
        if option == "FÄƒrÄƒ Valori Extreme":
            st.write(var2[["Sales Volume", "Price"]].describe())
        else:
            st.write(var3[["Sales Volume", "Price"]].describe())

if section == "Codificare È™i scalare":
    st.header("ðŸ§® Codificare È™i scalare")

    categorical_cols = ['Brand', 'Category', 'Color']
    numeric_cols = ['Price', 'Sales Volume']

    #label encoding
    st.subheader("ðŸŒ Codificare Label Encoding")
    label_encoded = data.copy()
    for col in categorical_cols:
        if col in label_encoded.columns:
            le = LabelEncoder()
            label_encoded[col] = le.fit_transform(label_encoded[col].astype(str))
    st.dataframe(label_encoded[categorical_cols].head())

    #one-hot encoding
    st.subheader("ðŸ”¢ Codificare One-Hot Encoding")
    onehot_encoded = pd.get_dummies(data[categorical_cols])
    st.dataframe(onehot_encoded.head())

    #frequency encoding
    st.subheader("ðŸ“Š Codificare Frequency Encoding")
    freq_encoded = data.copy()
    for col in categorical_cols:
        freq_map = freq_encoded[col].value_counts(normalize=True)
        freq_encoded[col + '_FreqEnc'] = freq_encoded[col].map(freq_map)
    st.dataframe(freq_encoded[[col + '_FreqEnc' for col in categorical_cols]].head())

    st.subheader("ðŸ“ Standardizare È™i Normalizare")
    scaler_standard = StandardScaler()
    scaler_minmax = MinMaxScaler()

    scaled_standard = scaler_standard.fit_transform(data[numeric_cols])
    scaled_minmax = scaler_minmax.fit_transform(data[numeric_cols])

    df_standard = pd.DataFrame(scaled_standard, columns=[f"{col}_std" for col in numeric_cols])
    df_minmax = pd.DataFrame(scaled_minmax, columns=[f"{col}_minmax" for col in numeric_cols])

    st.write("ðŸ”¹ StandardScaler (media=0, std=1):")
    st.dataframe(df_standard.head())

    st.write("ðŸ”¹ MinMaxScaler ([0,1]):")
    st.dataframe(df_minmax.head())

    st.subheader("ðŸ“ˆ VizualizÄƒri grafice")

    fig1, ax1 = plt.subplots()
    sns.histplot(data['Price'], bins=30, kde=True, ax=ax1)
    ax1.set_title('DistribuÈ›ia PreÈ›ului')
    st.pyplot(fig1)
    st.markdown("""
     ðŸ” **Interpretare**: PreÈ›urile sunt distribuite aproape uniform, ceea ce sugereazÄƒ o strategie de preÈ› variatÄƒ Ã®n portofoliul de produse.
     """)

    fig2, ax2 = plt.subplots()
    sns.histplot(data['Sales Volume'], bins=30, kde=True, ax=ax2, color="orange")
    ax2.set_title('DistribuÈ›ia Volumului de VÃ¢nzÄƒri')
    st.pyplot(fig2)
    st.markdown("""
     ðŸ” **Interpretare**: VÃ¢nzÄƒrile sunt relativ echilibrate Ã®ntre produse, dar cu variaÈ›ii uÈ™oare care pot reflecta sezonalitate sau preferinÈ›e.
     """)

    fig3, ax3 = plt.subplots(figsize=(10, 6))
    sns.boxplot(x='Category', y='Price', data=data, ax=ax3)
    ax3.set_title('Boxplot PreÈ› pe Categorii')
    st.pyplot(fig3)
    st.markdown("""
     ðŸ” **Interpretare**: Unele categorii (ex: Jewelry, Outerwear) tind sÄƒ aibÄƒ preÈ›uri mai ridicate, ceea ce reflectÄƒ poziÈ›ionarea diferitÄƒ pe piaÈ›Äƒ.
     """)

    fig4, ax4 = plt.subplots(figsize=(10, 6))
    sns.violinplot(x='Color', y='Sales Volume', data=data, ax=ax4)
    ax4.set_title('DistribuÈ›ia VÃ¢nzÄƒrilor pe Culoare (Violin plot)')
    st.pyplot(fig4)
    st.markdown("""
     ðŸ” **Interpretare**: DeÈ™i formele diferÄƒ, valorile medii ale vÃ¢nzÄƒrilor sunt similare pe culori. Culorile influenÈ›eazÄƒ distribuÈ›ia, dar nu drastic volumul total.
     """)

    fig5, ax5 = plt.subplots(figsize=(8, 5))
    corr = data[numeric_cols].corr()
    sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax5)
    ax5.set_title('Heatmap CorelaÈ›ii')
    st.pyplot(fig5)
    st.markdown("""
     ðŸ” **Interpretare**: CorelaÈ›ia dintre `Price` È™i `Sales Volume` este foarte micÄƒ (0.007), ceea ce sugereazÄƒ cÄƒ preÈ›ul nu influenÈ›eazÄƒ semnificativ volumul vÃ¢nzÄƒrilor.
     """)
if section == "Identificare si stergere duplicate":
    duplicates = data.duplicated().sum()
    st.write(f"ðŸ“Œ NumÄƒr de duplicate: {duplicates}")
    if duplicates > 0:
        if st.button("Sterge duplicate"):
            data.drop_duplicates(inplace=True)
            st.success("âœ… Duplicatele au fost sterse!")

if section == "Grupare dupa Categorie":
    sales_stats_by_category = data.groupby('Category')['Sales Volume'].agg(['sum', 'mean', 'min', 'max']).reset_index()
    price_stats_by_category = data.groupby('Category')['Price'].agg(['sum', 'mean', 'min', 'max']).reset_index()

    st.title("Analiza pe 'Categorie'")
    st.subheader("Statistici 'Sales Volume' per 'Categorie'")
    st.dataframe(sales_stats_by_category)
    st.subheader("Statistici 'Price' per 'Categorie'")
    st.dataframe(price_stats_by_category)

if section == "Predictie pret":
    st.header("ðŸ§  PredicÈ›ie Pret Produs cu ML")
    data['Price_log'] = np.log1p(data['Price'])

    df_encoded = pd.get_dummies(data.copy(), drop_first=True)
    target = 'Price_log'

    X = df_encoded.drop(columns=[target])
    y = df_encoded[target]
    X = X.select_dtypes(include=[np.number])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    lr_model = LinearRegression()
    lr_model.fit(X_train, y_train)
    lr_pred = lr_model.predict(X_test)
    lr_mae = mean_absolute_error(y_test, lr_pred)
    lr_mse = mean_squared_error(y_test, lr_pred)
    lr_r2 = r2_score(y_test, lr_pred)

    #random forest
    rf_model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features='sqrt',
        random_state=42
    )
    rf_model.fit(X_train, y_train)
    rf_pred = rf_model.predict(X_test)
    rf_mae = mean_absolute_error(y_test, rf_pred)
    rf_mse = mean_squared_error(y_test, rf_pred)
    rf_r2 = r2_score(y_test, rf_pred)

    #xgboots
    xgb_model = xgb.XGBRegressor(
        n_estimators=100,
        max_depth=4,
        learning_rate=0.1,
        subsample=0.7,
        colsample_bytree=0.7,
        gamma=1,
        reg_alpha=0.5,
        reg_lambda=1.0,
        random_state=42
    )
    xgb_model.fit(X_train, y_train)
    xgb_pred = xgb_model.predict(X_test)
    xgb_mae = mean_absolute_error(y_test, xgb_pred)
    xgb_mse = mean_squared_error(y_test, xgb_pred)
    xgb_r2 = r2_score(y_test, xgb_pred)

    st.subheader("ðŸ“Š Rezultatele modelelor de regresie")

    st.markdown(f"""
    ### ðŸ”¹ Linear Regression
    - MAE: {lr_mae:.2f}  
    - MSE: {lr_mse:.2f}  
    - RÂ²: {lr_r2:.3f}
    """)

    st.markdown(f"""
    ### ðŸŒ² Random Forest
    - MAE: {rf_mae:.2f}  
    - MSE: {rf_mse:.2f}  
    - RÂ²: {rf_r2:.3f}
    """)

    st.markdown(f"""
    ### âš¡ XGBoost
    - MAE: {xgb_mae:.2f}  
    - MSE: {xgb_mse:.2f}  
    - RÂ²: {xgb_r2:.3f}
    """)

    st.success(
        "âœ… Compararea modelelor este completÄƒ. Cel mai bun scor RÂ² indicÄƒ cel mai performant model pe datele actuale.")
    st.info("â„¹ï¸ S-a aplicat transformarea logaritmicÄƒ pe preÈ›.")

    st.markdown("""
    ### ðŸ§  Interpretare rezultate:
    - ðŸ”¹ **Linear Regression** a obÈ›inut un scor RÂ² de `0.931`, ceea ce Ã®nseamnÄƒ cÄƒ explicÄƒ aproximativ 93% din variaÈ›ia datelor. Acesta este un rezultat foarte bun pentru un model simplu.
    - ðŸŒ² **Random Forest** a obÈ›inut un scor RÂ² de `0.994`, ceea ce indicÄƒ o potrivire foarte bunÄƒ cu datele, avÃ¢nd erori mai mici.
    - âš¡ **XGBoost** a obÈ›inut cel mai bun scor, cu un RÂ² de `0.998`, ceea ce sugereazÄƒ o potrivire aproape perfectÄƒ pe datele de antrenament.

    ðŸ” **Recomandare**: Modelele precum **Random Forest** È™i **XGBoost** cu scoruri foarte mari de RÂ² pot indica **overfitting**. Este recomandat sÄƒ foloseÈ™ti tehnici de validare Ã®ncruciÈ™atÄƒ (cross-validation) È™i sÄƒ testezi pe un set extern de date pentru a evalua generalizarea corectÄƒ a modelelor.
    """)


if section == "AnalizÄƒ GeograficÄƒ":
    st.title("ðŸŒ Vizualizare GeograficÄƒ a VÃ¢nzÄƒrilor")

    world = gpd.read_file("dataIN/countries.geo.json")

    sales_by_country = data.groupby('Country')['Sales Volume'].sum().reset_index()
    geo_df = world.merge(sales_by_country, how='left', left_on='name', right_on='Country')

    fig, ax = plt.subplots(figsize=(15, 10))
    world.boundary.plot(ax=ax, linewidth=0.8, color='black')
    geo_df.plot(column='Sales Volume', ax=ax, legend=True, cmap='OrRd',
                missing_kwds={"color": "lightgrey", "label": "FÄƒrÄƒ date"})
    ax.set_title('DistribuÈ›ia VÃ¢nzÄƒrilor pe ÈšÄƒri', fontsize=18)
    st.pyplot(fig)



    data["Region"] = data["Country"].apply(assign_region)
    region_stats = data.groupby('Region')['Sales Volume'].sum().reset_index()

    fig2, ax2 = plt.subplots(figsize=(10, 6))
    sns.barplot(x='Sales Volume', y='Region', data=region_stats, palette='coolwarm', ax=ax2)
    ax2.set_title("VÃ¢nzÄƒri totale pe regiuni")
    st.pyplot(fig2)

    st.markdown("""
    ### ðŸ§  Interpretare Vizualizare GeograficÄƒ

    ðŸŒ **DistribuÈ›ia vÃ¢nzÄƒrilor pe È›Äƒri**:
    - Harta evidenÈ›iazÄƒ È›Äƒrile Ã®n care s-au Ã®nregistrat cele mai mari volume de vÃ¢nzÄƒri.
    - ÈšÄƒri precum **China**, **Statele Unite**, **Canada**, **Germania** È™i **Japonia** prezintÄƒ cele mai ridicate valori.
    - Culorile mai Ã®nchise indicÄƒ volume mai mari de vÃ¢nzÄƒri, Ã®n timp ce zonele gri nu conÈ›in date.

    ðŸ“Š **VÃ¢nzÄƒri totale pe regiuni**:
    - **Europa de Vest** este liderul vÃ¢nzÄƒrilor, cu un volum net superior altor regiuni.
    - **Asia** È™i **America** ocupÄƒ urmÄƒtoarele poziÈ›ii, cu valori considerabile.
    - **Europa de Est** È™i zona etichetatÄƒ â€žAlteleâ€ (ex: Australia) Ã®nregistreazÄƒ volume mai scÄƒzute.
    """)
if section == "Clusterizare clienÈ›i":
    st.header("ðŸ‘¥ Segmentare clienÈ›i prin KMeans")
    df_kmeans = data[["Annual Income (k$)", "Spending Score"]].dropna()

    inertia = []
    for k in range(1, 11):
        km = KMeans(n_clusters=k, random_state=42)
        km.fit(df_kmeans)
        inertia.append(km.inertia_)

    fig, ax = plt.subplots()
    sns.lineplot(x=range(1, 11), y=inertia, marker='o', ax=ax)
    ax.set_title("Metoda cotului pentru alegerea numÄƒrului de clustere")
    st.pyplot(fig)

    n_clusters = st.slider("Alege numÄƒrul de clustere", 2, 10, 3)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    df_kmeans["Cluster"] = kmeans.fit_predict(df_kmeans)

    fig2, ax2 = plt.subplots()
    sns.scatterplot(data=df_kmeans, x="Annual Income (k$)", y="Spending Score", hue="Cluster", palette="Set2", ax=ax2)
    ax2.set_title(f"Rezultatul clusterizÄƒrii Ã®n {n_clusters} clustere")
    st.pyplot(fig2)


if section == "Clasificare logisticÄƒ":
    st.header("ðŸ” Regresie logisticÄƒ â€“ clasificare clienÈ›i")

    data_class = data.copy()

    threshold = data_class['Spending Score'].quantile(0.75)  # sau 70, dar mai flexibil
    data_class['HighSpender'] = (data_class['Spending Score'] > threshold).astype(int)

    unique_classes = data_class['HighSpender'].nunique()
    if unique_classes < 2:
        st.error("âš ï¸ Clasificarea logisticÄƒ nu poate fi realizatÄƒ: toate valorile din 'HighSpender' sunt identice.")
    else:
        features = pd.get_dummies(data_class[['Genre', 'Age', 'Annual Income (k$)']], drop_first=True)
        target = data_class['HighSpender']

        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

        logreg = LogisticRegression()
        logreg.fit(X_train, y_train)
        y_pred = logreg.predict(X_test)

        st.write(f"ðŸ”¹ AcurateÈ›ea modelului: {logreg.score(X_test, y_test):.2f}")
        st.write("âœ… Clasificarea a fost realizatÄƒ cu succes.")


    st.write(f"ðŸ”¹ AcurateÈ›ea modelului: {logreg.score(X_test, y_test):.2f}")

if section == "Regresie multiplÄƒ":
    st.header("ðŸ“ˆ Regresie multiplÄƒ â€“ analizÄƒ cu Statsmodels")

    df_stats = data.dropna()
    df_stats = pd.get_dummies(df_stats, drop_first=True)

    X = df_stats[["Age", "Annual Income (k$)", "Genre_Male"]]
    X = sm.add_constant(X)
    X = X.astype(float)

    y = df_stats["Spending Score"].astype(float)

    model = sm.OLS(y, X).fit()
    st.text(model.summary())



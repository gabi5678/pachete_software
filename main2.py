import pandas as pd
import numpy as np
import streamlit as st
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import geopandas as gpd
from sklearn.cluster import KMeans
from sklearn.linear_model import LogisticRegression
import statsmodels.api as sm


from functii import detect_outliers_iqr, plot_boxplots,assign_region

data = pd.read_excel('dataIN/fashion.xlsx',sheet_name='Sheet1',index_col=0)
data["Annual Income (k$)"] = data["Price"] * 10 + 30
data["Spending Score"] = data["Sales Volume"] + np.random.randint(-10, 10, size=len(data))
data["Genre"] = np.random.choice(["Male", "Female"], size=len(data))
data["Age"] = np.random.randint(18, 65, size=len(data))

st.markdown("""
<style>
.title{
    color:#fa82f0 !important;
}
.custom-title {
    color: #b5029d !important;
}
</style>
""", unsafe_allow_html=True)

st.markdown('<h1 class="title">Proiect Analizare V√¢nzƒÉri Fashion</h1>',unsafe_allow_html=True)
st.markdown('<h3 class="custom-title">Antonescu Gabriela & Guguloi Iulia</h3>', unsafe_allow_html=True)

# Meniu de navigare lateral
section = st.sidebar.radio("Naviga»õi la:",
                           ["Descriere dataset",
                            "Informatii privind DataFrame-ul",
                            "Valori lipsa ale datasetului",
                            "Tratare valori lipsa",
                            "Analiza valorilor extreme",
                            "Codificare »ôi scalare",
                            "Identificare si stergere duplicate",
                            "Grupare dupa Categorie",
                            "Predictie pret",
                            "AnalizƒÉ GeograficƒÉ",
                            "Clusterizare clien»õi",
                            "Clasificare logisticƒÉ",
                            "Regresie multiplƒÉ",
                            ], key="radio_navigare")

if section == "Descriere dataset":
    st.header("üìä Analiza datasetului Fashion Sales")
    st.markdown("""
    ### üõçÔ∏è Despre Fashion Sales Dataset
    **Fashion Sales Dataset** este un set de date detaliat, conceput pentru a simula procesul real de v√¢nzƒÉri √Æn industria modei. Acesta oferƒÉ o perspectivƒÉ valoroasƒÉ asupra pie»õei modei »ôi reprezintƒÉ o resursƒÉ esen»õialƒÉ pentru analiza tendin»õelor de v√¢nzare, prognoza cererii »ôi optimizarea strategiilor de afaceri.

    üìå **Caracteristici principale:**
    - üõí **Proces de v√¢nzƒÉri realist** ‚Äì Include detalii despre achizi»õiile clien»õilor, tranzac»õii »ôi caracteristicile produselor.
    - üìÜ **ActualizƒÉri sƒÉptƒÉm√¢nale** ‚Äì ReflectƒÉ cele mai recente tendin»õe »ôi preferin»õe ale consumatorilor.
    - üè∑Ô∏è **Diverse atribute** ‚Äì Cuprinde informa»õii despre pre»õuri, branduri, categorii, rating-uri, review-uri, dimensiuni disponibile, culori »ôi istoric de achizi»õii.
    - üßë‚Äçü§ù‚Äçüßë **Interac»õiuni cu clien»õii** ‚Äì Con»õine date despre reviste de modƒÉ, influenceri »ôi impactul re»õelelor sociale asupra deciziilor de cumpƒÉrare.
    - üçÇ **Varia»õii sezoniere** ‚Äì Permite analiza preferin»õelor de modƒÉ √Æn func»õie de anotimpuri »ôi perioade specifice din zi.
    - üìä **E»ôantion semnificativ** ‚Äì NumƒÉrul mare de observa»õii asigurƒÉ un set de date robust pentru analize detaliate »ôi prognoze precise.
    """)

if section == "Informatii privind DataFrame-ul":
    st.header('Dataset')
    st.dataframe(data.head(1000))

if section == "Valori lipsa ale datasetului":
    if data.isnull().sum().sum() == 0:
        st.success("‚úÖ Nu existƒÉ valori lipsƒÉ √Æn dataset!")
    else:
        st.warning("‚ö†Ô∏è ExistƒÉ valori lipsƒÉ!")
        fig, ax = plt.subplots(figsize=(10, 5))
        sns.heatmap(data.isnull(), cbar=False, cmap="viridis", ax=ax)
        st.pyplot(fig)

if section == "Tratare valori lipsa":
    st.header("üßº Tratarea valorilor lipsƒÉ")

    missing = data.isnull().sum()
    missing_percent = (missing / len(data)) * 100
    missing_df = pd.DataFrame({"Valori LipsƒÉ": missing, "Procent": missing_percent})
    missing_df = missing_df[missing_df["Valori LipsƒÉ"] > 0]

    if missing_df.empty:
        st.success("‚úÖ Nu existƒÉ valori lipsƒÉ de tratat!")
    else:
        st.dataframe(missing_df)

        metoda_selectata = st.radio(
            "Alege metoda de tratare:",
            ("Completare cu mediana/moda", "Eliminare r√¢nduri cu valori lipsƒÉ", "Completare cu media"),
            key="radio_metoda_valori_lipsa"
        )

        if metoda_selectata == "Completare cu mediana/moda":
            data_imputed = data.copy()
            for col in data.columns:
                if data[col].isnull().sum() > 0:
                    if data[col].dtype == 'object':
                        data_imputed[col] = data[col].fillna(data[col].mode()[0])
                    else:
                        data_imputed[col] = data[col].fillna(data[col].median())
            st.success("‚úÖ Valorile lipsƒÉ au fost completate (mediana sau moda, dupƒÉ tipul coloanei)")
            st.dataframe(data_imputed.head())

        elif metoda_selectata == "Eliminare r√¢nduri cu valori lipsƒÉ":
            data_dropped = data.dropna()
            st.warning(f"‚ö†Ô∏è S-au eliminat {len(data) - len(data_dropped)} r√¢nduri care con»õineau valori lipsƒÉ.")
            st.dataframe(data_dropped.head())

        elif metoda_selectata == "Completare cu media":
            data_mean_imputed = data.copy()
            for col in data.columns:
                if data[col].isnull().sum() > 0:
                    if data[col].dtype != 'object':
                        data_mean_imputed[col] = data[col].fillna(data[col].mean())
            st.success("‚úÖ Valorile lipsƒÉ au fost completate cu media (doar coloane numerice).")
            st.dataframe(data_mean_imputed.head())


if section == "Analiza valorilor extreme":
    st.title("üìä Analiza Valorilor Extreme - Fashion Sales")

    st.markdown("""
    **Ce sunt valorile extreme?**  
    Valorile extreme sunt punctele de date care sunt semnificativ mai mari sau mai mici dec√¢t restul observa»õiilor. Acestea pot afecta analiza statisticƒÉ »ôi rezultatele modelului nostru, de aceea este important sƒÉ le gestionƒÉm corespunzƒÉtor.

    **Cum le detectƒÉm?**  
    Folosim metoda **Interquartile Range (IQR)**, care identificƒÉ valori extreme ca fiind cele care depƒÉ»ôesc limita inferioarƒÉ »ôi superioarƒÉ:

    - **Q1 (quartila 25%)** ‚Äì valoarea sub care se aflƒÉ 25% dintre date
    - **Q3 (quartila 75%)** ‚Äì valoarea sub care se aflƒÉ 75% dintre date
    - **IQR = Q3 - Q1**
    - **Limita inferioarƒÉ** = Q1 - 1.5 * IQR
    - **Limita superioarƒÉ** = Q3 + 1.5 * IQR

    Orice valoare √Æn afara acestui interval este consideratƒÉ **extremƒÉ**.
    """)
    sales_lb, sales_ub = detect_outliers_iqr(data, "Sales Volume")
    price_lb, price_ub = detect_outliers_iqr(data, "Price")

    st.write(f"üîπ **Limite pentru `Sales Volume`**: [{sales_lb:.2f}, {sales_ub:.2f}]")
    st.write(f"üîπ **Limite pentru `Price`**: [{price_lb:.2f}, {price_ub:.2f}]")

    #pastram datele
    var1 = data.copy()

    #eliminam valorile extreme
    var2 = data[(data["Sales Volume"] >= sales_lb) & (data["Sales Volume"] <= sales_ub) &
                (data["Price"] >= price_lb) & (data["Price"] <= price_ub)]

    #inlocuim valorile extreme cu percentila 95%
    var3 = data.copy()
    var3.loc[var3["Sales Volume"] > sales_ub, "Sales Volume"] = data["Sales Volume"].quantile(0.95)
    var3.loc[var3["Price"] > price_ub, "Price"] = data["Price"].quantile(0.95)

    option = st.radio(
        "Alege varianta de tratare a valorilor extreme:",
        ("Original", "Fara Valori Extreme", "Inlocuire cu Percentila 95%"),
        key="radio_analiza_valori_extreme"
    )

    if option == "Original":
        st.markdown("""
        **Varianta `Original` pastreaza toate datele a»ôa cum sunt, fara a elimina sau modifica valori extreme.**
        Aceasta poate fi utila atunci cand vrem sa vedem intreaga distributie a datelor.
        """)
        st.pyplot(plot_boxplots(var1, "Original"))
        st.write(f"üìä Numar total de observa»õii: {len(var1)}")

    elif option == "Fara Valori Extreme":
        st.markdown("""
        **Varianta `FƒÉrƒÉ Valori Extreme` eliminƒÉ toate observa»õiile care depƒÉ»ôesc limitele definite prin metoda IQR.**
        Aceasta este utilƒÉ dacƒÉ dorim sƒÉ avem o distribu»õie mai curatƒÉ a datelor, fƒÉrƒÉ valori care ar putea distorsiona analiza.
        """)
        st.pyplot(plot_boxplots(var2, "FƒÉrƒÉ Valori Extreme"))
        st.write(f"üìâ NumƒÉr de observa»õii dupƒÉ eliminare: {len(var2)}")
        st.write(f"üöÄ Observa»õii eliminate: {len(var1) - len(var2)}")

    elif option == "Inlocuire cu Percentila 95%":
        st.markdown("""
        **Varianta `√énlocuire cu Percentila 95%` modificƒÉ valorile extreme peste limitƒÉ cu valoarea percentilei 95%.**
        Aceasta este o metodƒÉ mai pu»õin agresivƒÉ dec√¢t eliminarea completƒÉ a datelor »ôi poate fi utilƒÉ atunci c√¢nd dorim sƒÉ pƒÉstrƒÉm informa»õia generalƒÉ.
        """)
        st.pyplot(plot_boxplots(var3, "√énlocuire cu Percentila 95%"))
        st.write(f"‚úÖ √énlocuire realizatƒÉ pentru valorile extreme peste percentila 95%")

    st.subheader("üìä Statistici descriptive")
    st.write("üîπ **Original**")
    st.write(var1[["Sales Volume", "Price"]].describe())

    if option != "Original":
        st.write("üîπ **Noua varianta de date**")
        if option == "FƒÉrƒÉ Valori Extreme":
            st.write(var2[["Sales Volume", "Price"]].describe())
        else:
            st.write(var3[["Sales Volume", "Price"]].describe())

if section == "Codificare »ôi scalare":
    st.header("üßÆ Codificare »ôi scalare")

    categorical_cols = ['Brand', 'Category', 'Color']
    numeric_cols = ['Price', 'Sales Volume']

    #label encoding
    st.subheader("üåê Codificare Label Encoding")
    label_encoded = data.copy()
    for col in categorical_cols:
        if col in label_encoded.columns:
            le = LabelEncoder()
            label_encoded[col] = le.fit_transform(label_encoded[col].astype(str))
    st.dataframe(label_encoded[categorical_cols].head())

    #one-hot encoding
    st.subheader("üî¢ Codificare One-Hot Encoding")
    onehot_encoded = pd.get_dummies(data[categorical_cols])
    st.dataframe(onehot_encoded.head())

    #frequency encoding
    st.subheader("üìä Codificare Frequency Encoding")
    freq_encoded = data.copy()
    for col in categorical_cols:
        freq_map = freq_encoded[col].value_counts(normalize=True)
        freq_encoded[col + '_FreqEnc'] = freq_encoded[col].map(freq_map)
    st.dataframe(freq_encoded[[col + '_FreqEnc' for col in categorical_cols]].head())

    st.subheader("üìè Standardizare »ôi Normalizare")
    scaler_standard = StandardScaler()
    scaler_minmax = MinMaxScaler()

    scaled_standard = scaler_standard.fit_transform(data[numeric_cols])
    scaled_minmax = scaler_minmax.fit_transform(data[numeric_cols])

    df_standard = pd.DataFrame(scaled_standard, columns=[f"{col}_std" for col in numeric_cols])
    df_minmax = pd.DataFrame(scaled_minmax, columns=[f"{col}_minmax" for col in numeric_cols])

    st.write("üîπ StandardScaler (media=0, std=1):")
    st.dataframe(df_standard.head())

    st.write("üîπ MinMaxScaler ([0,1]):")
    st.dataframe(df_minmax.head())

    st.subheader("üìà VizualizƒÉri grafice")

    fig1, ax1 = plt.subplots()
    sns.histplot(data['Price'], bins=30, kde=True, ax=ax1)
    ax1.set_title('Distribu»õia Pre»õului')
    st.pyplot(fig1)
    st.markdown("""
     üîç **Interpretare**: Pre»õurile sunt distribuite aproape uniform, ceea ce sugereazƒÉ o strategie de pre»õ variatƒÉ √Æn portofoliul de produse.
     """)

    fig2, ax2 = plt.subplots()
    sns.histplot(data['Sales Volume'], bins=30, kde=True, ax=ax2, color="orange")
    ax2.set_title('Distribu»õia Volumului de V√¢nzƒÉri')
    st.pyplot(fig2)
    st.markdown("""
     üîç **Interpretare**: V√¢nzƒÉrile sunt relativ echilibrate √Æntre produse, dar cu varia»õii u»ôoare care pot reflecta sezonalitate sau preferin»õe.
     """)

    fig3, ax3 = plt.subplots(figsize=(10, 6))
    sns.boxplot(x='Category', y='Price', data=data, ax=ax3)
    ax3.set_title('Boxplot Pre»õ pe Categorii')
    st.pyplot(fig3)
    st.markdown("""
     üîç **Interpretare**: Unele categorii (ex: Jewelry, Outerwear) tind sƒÉ aibƒÉ pre»õuri mai ridicate, ceea ce reflectƒÉ pozi»õionarea diferitƒÉ pe pia»õƒÉ.
     """)

    fig4, ax4 = plt.subplots(figsize=(10, 6))
    sns.violinplot(x='Color', y='Sales Volume', data=data, ax=ax4)
    ax4.set_title('Distribu»õia V√¢nzƒÉrilor pe Culoare (Violin plot)')
    st.pyplot(fig4)
    st.markdown("""
     üîç **Interpretare**: De»ôi formele diferƒÉ, valorile medii ale v√¢nzƒÉrilor sunt similare pe culori. Culorile influen»õeazƒÉ distribu»õia, dar nu drastic volumul total.
     """)

    fig5, ax5 = plt.subplots(figsize=(8, 5))
    corr = data[numeric_cols].corr()
    sns.heatmap(corr, annot=True, cmap='coolwarm', ax=ax5)
    ax5.set_title('Heatmap Corela»õii')
    st.pyplot(fig5)
    st.markdown("""
     üîç **Interpretare**: Corela»õia dintre `Price` »ôi `Sales Volume` este foarte micƒÉ (0.007), ceea ce sugereazƒÉ cƒÉ pre»õul nu influen»õeazƒÉ semnificativ volumul v√¢nzƒÉrilor.
     """)
if section == "Identificare si stergere duplicate":
    duplicates = data.duplicated().sum()
    st.write(f"üìå NumƒÉr de duplicate: {duplicates}")
    if duplicates > 0:
        if st.button("Sterge duplicate"):
            data.drop_duplicates(inplace=True)
            st.success("‚úÖ Duplicatele au fost sterse!")

if section == "Grupare dupa Categorie":
    sales_stats_by_category = data.groupby('Category')['Sales Volume'].agg(['sum', 'mean', 'min', 'max']).reset_index()
    price_stats_by_category = data.groupby('Category')['Price'].agg(['sum', 'mean', 'min', 'max']).reset_index()

    st.title("Analiza pe 'Categorie'")
    st.subheader("Statistici 'Sales Volume' per 'Categorie'")
    st.dataframe(sales_stats_by_category)
    st.subheader("Statistici 'Price' per 'Categorie'")
    st.dataframe(price_stats_by_category)

if section == "Predictie pret":
    st.header("üß† Predic»õie Pret Produs cu ML")
    data['Price_log'] = np.log1p(data['Price'])

    df_encoded = pd.get_dummies(data.copy(), drop_first=True)
    target = 'Price_log'

    X = df_encoded.drop(columns=[target])
    y = df_encoded[target]
    X = X.select_dtypes(include=[np.number])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    lr_model = LinearRegression()
    lr_model.fit(X_train, y_train)
    lr_pred = lr_model.predict(X_test)
    lr_mae = mean_absolute_error(y_test, lr_pred)
    lr_mse = mean_squared_error(y_test, lr_pred)
    lr_r2 = r2_score(y_test, lr_pred)

    #random forest
    rf_model = RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=10,
        min_samples_leaf=5,
        max_features='sqrt',
        random_state=42
    )
    rf_model.fit(X_train, y_train)
    rf_pred = rf_model.predict(X_test)
    rf_mae = mean_absolute_error(y_test, rf_pred)
    rf_mse = mean_squared_error(y_test, rf_pred)
    rf_r2 = r2_score(y_test, rf_pred)

    #xgboots
    xgb_model = xgb.XGBRegressor(
        n_estimators=100,
        max_depth=4,
        learning_rate=0.1,
        subsample=0.7,
        colsample_bytree=0.7,
        gamma=1,
        reg_alpha=0.5,
        reg_lambda=1.0,
        random_state=42
    )
    xgb_model.fit(X_train, y_train)
    xgb_pred = xgb_model.predict(X_test)
    xgb_mae = mean_absolute_error(y_test, xgb_pred)
    xgb_mse = mean_squared_error(y_test, xgb_pred)
    xgb_r2 = r2_score(y_test, xgb_pred)

    st.subheader("üìä Rezultatele modelelor de regresie")

    st.markdown(f"""
    ### üîπ Linear Regression
    - MAE: {lr_mae:.2f}  
    - MSE: {lr_mse:.2f}  
    - R¬≤: {lr_r2:.3f}
    """)

    st.markdown(f"""
    ### üå≤ Random Forest
    - MAE: {rf_mae:.2f}  
    - MSE: {rf_mse:.2f}  
    - R¬≤: {rf_r2:.3f}
    """)

    st.markdown(f"""
    ### ‚ö° XGBoost
    - MAE: {xgb_mae:.2f}  
    - MSE: {xgb_mse:.2f}  
    - R¬≤: {xgb_r2:.3f}
    """)

    st.success(
        "‚úÖ Compararea modelelor este completƒÉ. Cel mai bun scor R¬≤ indicƒÉ cel mai performant model pe datele actuale.")
    st.info("‚ÑπÔ∏è S-a aplicat transformarea logaritmicƒÉ pe pre»õ.")

    st.markdown("""
    ### üß† Interpretare rezultate:
    - üîπ **Linear Regression** a ob»õinut un scor R¬≤ de `0.931`, ceea ce √ÆnseamnƒÉ cƒÉ aproximativ 93% din varia»õia pre»õurilor este explicatƒÉ de model. MAE-ul de `0.13` sugereazƒÉ o eroare medie destul de micƒÉ, ceea ce e foarte bun pentru un model liniar de bazƒÉ.

    - üå≤ **Random Forest** a ob»õinut un scor R¬≤ excep»õional de `0.999`, cu o eroare MAE de doar `0.01`. Acest lucru indicƒÉ o potrivire aproape perfectƒÉ pe datele de testare, ceea ce ar putea fi un semn de **overfitting** (modelul a √ÆnvƒÉ»õat prea bine datele).

    - ‚ö° **XGBoost** are un scor R¬≤ de `0.998`, foarte apropiat de cel al Random Forest, cu aceea»ôi eroare medie (`0.01`). Este un rezultat excelent, confirm√¢nd performan»õa ridicatƒÉ a acestui model avansat.

    üîç **Recomandare**:
    At√¢t **Random Forest** c√¢t »ôi **XGBoost** oferƒÉ performan»õe foarte mari, dar aceste scoruri aproape perfecte pot fi √Æn»ôelƒÉtoare. """)

if section == "AnalizƒÉ GeograficƒÉ":
    st.title("üåç Vizualizare GeograficƒÉ a V√¢nzƒÉrilor")

    world = gpd.read_file("dataIN/countries.geo.json")

    sales_by_country = data.groupby('Country')['Sales Volume'].sum().reset_index()
    geo_df = world.merge(sales_by_country, how='left', left_on='name', right_on='Country')

    fig, ax = plt.subplots(figsize=(15, 10))
    world.boundary.plot(ax=ax, linewidth=0.8, color='black')
    geo_df.plot(column='Sales Volume', ax=ax, legend=True, cmap='OrRd',
                missing_kwds={"color": "lightgrey", "label": "FƒÉrƒÉ date"})
    ax.set_title('Distribu»õia V√¢nzƒÉrilor pe »öƒÉri', fontsize=18)
    st.pyplot(fig)



    data["Region"] = data["Country"].apply(assign_region)
    region_stats = data.groupby('Region')['Sales Volume'].sum().reset_index()

    fig2, ax2 = plt.subplots(figsize=(10, 6))
    sns.barplot(x='Sales Volume', y='Region', data=region_stats, palette='coolwarm', ax=ax2)
    ax2.set_title("V√¢nzƒÉri totale pe regiuni")
    st.pyplot(fig2)

    st.markdown("""
    ### üß† Interpretare Vizualizare GeograficƒÉ

    üåç **Distribu»õia v√¢nzƒÉrilor pe »õƒÉri**:
    - Harta eviden»õiazƒÉ »õƒÉrile √Æn care s-au √Ænregistrat cele mai mari volume de v√¢nzƒÉri.
    - »öƒÉri precum **China**, **Statele Unite**, **Canada**, **Germania** »ôi **Japonia** prezintƒÉ cele mai ridicate valori.
    - Culorile mai √Ænchise indicƒÉ volume mai mari de v√¢nzƒÉri, √Æn timp ce zonele gri nu con»õin date.

    üìä **V√¢nzƒÉri totale pe regiuni**:
    - **Europa de Vest** este liderul v√¢nzƒÉrilor, cu un volum net superior altor regiuni.
    - **Asia** »ôi **America** ocupƒÉ urmƒÉtoarele pozi»õii, cu valori considerabile.
    - **Europa de Est** »ôi zona etichetatƒÉ ‚ÄûAltele‚Äù (ex: Australia) √ÆnregistreazƒÉ volume mai scƒÉzute.
    """)
if section == "Clusterizare clien»õi":
    st.header("üë• Segmentare clien»õi prin KMeans")
    df_kmeans = data[["Annual Income (k$)", "Spending Score"]].dropna()

    inertia = []
    for k in range(1, 11):
        km = KMeans(n_clusters=k, random_state=42)
        km.fit(df_kmeans)
        inertia.append(km.inertia_)

    fig, ax = plt.subplots()
    sns.lineplot(x=range(1, 11), y=inertia, marker='o', ax=ax)
    ax.set_title("Metoda cotului pentru alegerea numƒÉrului de clustere")
    st.pyplot(fig)

    n_clusters = st.slider("Alege numƒÉrul de clustere", 2, 10, 3)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    df_kmeans["Cluster"] = kmeans.fit_predict(df_kmeans)

    fig2, ax2 = plt.subplots()
    sns.scatterplot(data=df_kmeans, x="Annual Income (k$)", y="Spending Score", hue="Cluster", palette="Set2", ax=ax2)
    ax2.set_title(f"Rezultatul clusterizƒÉrii √Æn {n_clusters} clustere")
    st.pyplot(fig2)

    st.markdown("""
    ### üß† Interpretare clusterizare KMeans

    üìå **Metoda cotului (Elbow Method):**
    - Graficul eviden»õiazƒÉ punctul de inflexiune √Æn jurul valorii `k=3`, ceea ce sugereazƒÉ cƒÉ 3 este un numƒÉr optim de clustere.
    - Alegerea a mai mult de 3 clustere duce la o √ÆmbunƒÉtƒÉ»õire nesemnificativƒÉ a performan»õei »ôi poate complica analiza.

    üìä **Rezultatul clusterizƒÉrii √Æn 3 clustere:**
    - Fiecare client a fost grupat √Æn func»õie de **Venitul Anual** »ôi **Scorul de Cheltuieli**.
    - Clusterele rezultate sunt:
      - üü¢ **Cluster 0** ‚Äì clien»õi cu scor mare de cheltuieli: poten»õiali cumpƒÉrƒÉtori fideli sau segment VIP.
      - üü† **Cluster 1** ‚Äì clien»õi cu scor mediu: cumpƒÉrƒÉtori constan»õi, dar modera»õi.
      - üîµ **Cluster 2** ‚Äì clien»õi cu scor redus: cumpƒÉrƒÉtori ocazionali, posibil sensibili la pre»õ.

    üìà **Observa»õii:**
    - Distribu»õia pe axe aratƒÉ cƒÉ scorul de cheltuieli este principalul diferen»õiator √Æntre clustere.
    - Venitul anual are o influen»õƒÉ mai micƒÉ √Æn separarea grupurilor, semnal√¢nd cƒÉ atitudinea fa»õƒÉ de cheltuieli nu depinde direct de venit.


    """)


if section == "Clasificare logisticƒÉ":
    st.header("üîê Regresie logisticƒÉ ‚Äì clasificare clien»õi")

    data_class = data.copy()

    threshold = data_class['Spending Score'].quantile(0.75)
    data_class['HighSpender'] = (data_class['Spending Score'] > threshold).astype(int)

    unique_classes = data_class['HighSpender'].nunique()
    if unique_classes < 2:
        st.error("‚ö†Ô∏è Clasificarea logisticƒÉ nu poate fi realizatƒÉ: toate valorile din 'HighSpender' sunt identice.")
    else:
        features = pd.get_dummies(data_class[['Genre', 'Age', 'Annual Income (k$)']], drop_first=True)
        target = data_class['HighSpender']

        X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

        logreg = LogisticRegression()
        logreg.fit(X_train, y_train)
        y_pred = logreg.predict(X_test)

        st.write("‚úÖ Clasificarea a fost realizatƒÉ cu succes.")


    st.write(f"üîπ Acurate»õea modelului: {logreg.score(X_test, y_test):.2f}")
    st.markdown("""
    ### üß† Interpretare rezultate ‚Äì Regresie LogisticƒÉ

    üîç Modelul de regresie logisticƒÉ a fost antrenat pentru a **clasifica clien»õii √Æn func»õie de comportamentul lor de cumpƒÉrare**, mai exact dacƒÉ sunt sau nu ‚Äûmari cheltuitori‚Äù.

    üìä **Acurate»õea ob»õinutƒÉ: 0.75**
    - Acurate»õea de `75%` √ÆnseamnƒÉ cƒÉ modelul a clasificat corect 3 din 4 clien»õi din setul de test.
    - Este un rezultat **acceptabil**, indic√¢nd cƒÉ modelul are o capacitate decentƒÉ de generalizare.

    """)

if section == "Regresie multiplƒÉ":
    st.header("üìà Regresie multiplƒÉ ‚Äì analizƒÉ cu Statsmodels")

    df_stats = data.dropna()
    df_stats = pd.get_dummies(df_stats, drop_first=True)

    X = df_stats[["Age", "Annual Income (k$)", "Genre_Male"]]
    X = sm.add_constant(X)
    X = X.astype(float)

    y = df_stats["Spending Score"].astype(float)

    model = sm.OLS(y, X).fit()
    st.text(model.summary())


    st.markdown("""
    ### üìâ Interpretare regresie multiplƒÉ
    
    - üîπ **R¬≤ = 0.000** ‚Üí Modelul nu explicƒÉ deloc varia»õia scorului de cheltuieli.
    - üîπ Niciuna dintre variabile (`Age`, `Annual Income`, `Genre_Male`) **nu este semnificativƒÉ** (p-valori > 0.05).
    - ‚ö†Ô∏è Modelul **nu este util** pentru predic»õie √Æn forma actualƒÉ.
    - ‚úÖ Se recomandƒÉ explorarea altor variabile sau metode de modelare.
    """)
